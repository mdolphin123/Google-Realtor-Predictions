# -*- coding: utf-8 -*-
"""Past Prediction Notebook

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12VO6OB6NYvIBAzC-1MnVYhNhCLpnGZCZ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/sample_data/redfin_data.csv')
df.head(5)


df = df.drop(['LOCATION', '$/SQUARE FEET', 'DAYS ON MARKET', 'HOA/MONTH', 'SALE TYPE', 'ADDRESS', 'CITY', 'STATE OR PROVINCE', 'NEXT OPEN HOUSE START TIME', 'URL (SEE https://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)', 'SOURCE', 'MLS#', 'FAVORITE', 'INTERESTED', 'STATUS', 'NEXT OPEN HOUSE END TIME'], axis = 1)

#Reduce outliers in pricing
df = df[(df['PRICE'] > 100000) & (df['PRICE'] < 1750000)]
df = df.dropna(subset = ['PROPERTY TYPE', 'YEAR BUILT', 'SOLD DATE', 'BEDS', 'BATHS', 'LOT SIZE', 'SQUARE FEET'])
df = df[(df['PROPERTY TYPE'] != "Other") & (df['PROPERTY TYPE'] != "Unknown")]

df.head(5)

#Cleaning Data

#Dropping the ones that don't matter
df = df.drop(['$/SQUARE FEET', 'DAYS ON MARKET', 'HOA/MONTH', 'SALE TYPE', 'ADDRESS', 'CITY', 'STATE OR PROVINCE', 'NEXT OPEN HOUSE START TIME', 'URL (SEE https://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)', 'SOURCE', 'MLS#', 'FAVORITE', 'INTERESTED', 'STATUS', 'NEXT OPEN HOUSE END TIME'], axis = 1)

#Reduce outliers in pricing
df = df[(df['PRICE'] > 100000) & (df['PRICE'] < 1750000)]
df = df.dropna(subset = ['PROPERTY TYPE', 'YEAR BUILT', 'SOLD DATE', 'BEDS', 'BATHS', 'LOT SIZE', 'SQUARE FEET'])
df = df[(df['PROPERTY TYPE'] != "Other") & (df['PROPERTY TYPE'] != "Unknown")]
print(df.shape)

#dealing with zipcodes/location, convert zipcodes to property and violent crime
vc = {27712: 29.4, 27703: 49.7, 27701: 70.2, 27707: 53.5, 27713: 39.5, 27704: 61.4, 27705: 47.4, 27503: 36.9, 27709: 23.4, 27278: 30.3}
pc = {27712: 40.8, 27703: 64.4, 27701: 78.2, 27707: 67.8, 27713: 58.3, 27704: 72.4, 27705: 63.7, 27503: 46.2, 27709: 31.9, 27278: 40}
#Making sure to only use Durham Zipcodes
df = df[(df['ZIP OR POSTAL CODE'] == 27703) | (df['ZIP OR POSTAL CODE'] == 27713) | (df['ZIP OR POSTAL CODE'] == 27707) | (df['ZIP OR POSTAL CODE'] == 27705) | (df['ZIP OR POSTAL CODE'] == 27704) | (df['ZIP OR POSTAL CODE'] == 27701) | (df['ZIP OR POSTAL CODE'] == 27712) | (df['ZIP OR POSTAL CODE'] == 27503)]
df = pd.get_dummies(df, columns = ['PROPERTY TYPE'], dtype = 'int')

df['PROPERTY_CRIME'] = df['ZIP OR POSTAL CODE'].map(pc)
df['VIOLENT_CRIME'] = df['ZIP OR POSTAL CODE'].map(vc)

print(df['ZIP OR POSTAL CODE'].unique())

print(df.shape)
df = df.drop(['ZIP OR POSTAL CODE', 'LOCATION'], axis = 1)


#print(df.shape)


#for testing
#df = df.drop(['SOLD DATE'], axis = 1)
df.head(10)

"""**Dealing with Time Trend**"""

df['SOLD DATE'] = df['SOLD DATE'].astype('datetime64[ns]')
df['YEAR'] = df['SOLD DATE'].dt.year
df['MONTH'] = df['SOLD DATE'].dt.month


#df = pd.get_dummies(df, columns = ['YEAR'])
df = df.drop(columns = ['SOLD DATE'])

#more feature engineering
df['HOUSE AGE'] = df['YEAR'] - df['YEAR BUILT']

df = df.reset_index(drop=True)

df.head(30)

"""Future possibility incorporating the median price data as a feature"""

import matplotlib.pyplot as plt

df_time = pd.read_csv('/content/sample_data/zipcode_saleprice.csv')
df_time = df_time.drop(['RegionID', 'SizeRank', 'RegionType', 'StateName', 'State', 'City', 'Metro', 'CountyName'], axis = 1)
df_time = df_time.rename(columns = {'RegionName': 'ZIP OR POSTAL CODE'})
df_time.head(5)


temp_27703 = df_time.loc[0, '2001-02-01':]
temp_27713 = df_time.loc[1, '2001-02-01':]
temp_27707 = df_time.loc[2, '2001-02-01':]
temp_27705 = df_time.loc[3, '2001-02-01':]
temp_27704 = df_time.loc[4, '2001-02-01':]
temp_27701 = df_time.loc[5, '2001-02-01':]
temp_27712 = df_time.loc[6, '2001-02-01':]
temp_27503 = df_time.loc[7, '2001-02-01':]


map_27703 = {temp_27703.keys: temp_27703.values}
map_27713 = {temp_27713.keys: temp_27713.values}
map_27707 = {temp_27707.keys: temp_27707.values}
map_27705 = {temp_27705.keys: temp_27705.values}
map_27704 = {temp_27704.keys: temp_27704.values}
map_27701 = {temp_27701.keys: temp_27701.values}
map_27712 = {temp_27712.keys: temp_27712.values}
map_27503 = {temp_27503.keys: temp_27503.values}

"""**Simple Linear Regression Model**"""

Train_X, Test_X, Train_y, Test_y = train_test_split(df.drop('PRICE', axis=1), df['PRICE'], test_size=0.2, random_state=42)

Train_X = Train_X.reset_index(drop=True)
Test_X = Test_X.reset_index(drop=True)
Train_y = Train_y.reset_index(drop=True)
Test_y = Test_y.reset_index(drop=True)

#Make some verifications
print(Train_X.shape, Train_y.shape)
print(Test_X.shape, Test_y.shape)

df.isnull().sum()

from sklearn import preprocessing, svm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import root_mean_squared_error, mean_squared_error, mean_absolute_error, r2_score as r2


model = LinearRegression().fit(Train_X, Train_y)
predictions = model.predict(Test_X)


rmse_loss = root_mean_squared_error(predictions, Test_y)
mse_loss = mean_squared_error(predictions, Test_y)
mae_loss = mean_absolute_error(predictions, Test_y)
r2_loss = r2(predictions, Test_y)

print('RMSE:', rmse_loss)
print('MSE:', mse_loss)
print('MAE:', mae_loss)
print('R2:', r2_loss)

"""Making K-Folds (K = 10)"""

from sklearn.model_selection import KFold
kf = KFold(n_splits = 10, shuffle = True, random_state = 42) #consider putting shuffle to true, but rn we also want to consider time

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
model = LinearRegression()
#fit using model, which is the linear regression model!!!

scores_r2 = cross_val_score(model, Train_X, Train_y, cv=kf, scoring='r2')
scores_mse = cross_val_score(model, Train_X, Train_y, cv=kf, scoring='neg_mean_squared_error')
scores_mae = cross_val_score(model, Train_X, Train_y, cv=kf, scoring='neg_mean_absolute_error')
scores_rmse = cross_val_score(model, Train_X, Train_y, cv=kf, scoring='neg_root_mean_squared_error')

print('RMSE:', scores_rmse.mean())
print('MSE:', scores_mse.mean())
print('MAE:', scores_mae.mean())
print('R2:', scores_r2.mean())

Test_X.head(5)
#Train_y.head(5)

#Test_X.head(5)
#Test_y.head(5)

"""**XGBoost Model**"""

from xgboost import XGBRegressor
from sklearn.metrics import max_error

#Try a default model first
model = XGBRegressor(random_state=42, n_jobs=-1, n_estimators=1000, max_depth=3, learning_rate=0.1)

model.fit(Train_X, Train_y)
preds = model.predict(Test_X)


rmse_loss = root_mean_squared_error(preds, Test_y)
mse_loss = mean_squared_error(preds, Test_y)
mae_loss = mean_absolute_error(preds, Test_y)
r2_loss = r2(preds, Test_y)
max = max_error(preds, Test_y)


print('RMSE:', rmse_loss)
print('MSE:', mse_loss)
print('MAE:', mae_loss)
print('R2:', r2_loss)
print('Max Error:', max)

"""Tune Hyperparameters (RandomSearch)"""

from sklearn.model_selection import RandomizedSearchCV

param_grid = {'max_depth': [1,2,3,4,5,6], 'learning_rate': [0.1, 0.15, 0.2, 0.25, 0.3], 'n_estimators': np.arange(100, 1000, 25), 'reg_lambda': [1, 2, 3, 4, 5]}

model = XGBRegressor()

grid = RandomizedSearchCV(model, param_grid,scoring='neg_mean_absolute_percentage_error', cv=8, n_iter = 50)
grid.fit(Train_X, Train_y)

print(grid.best_params_)

from sklearn.metrics import mean_absolute_percentage_error
model_2 = XGBRegressor(random_state=42, n_jobs=-1, n_estimators=975, max_depth=6, learning_rate=0.15, reg_lambda = 4)

model_2.fit(Train_X, Train_y)
preds = model_2.predict(Test_X)

#print some metrics
print("mean absolute error:" + mean_absolute_error(preds, Test_y))
print("mean absolute percentage error:" + mean_absolute_percentage_error(preds, Test_y))
print(Test_y.shape)
print("\n")

#get the percentage errors of the houses
error = abs(preds - Test_y)/Test_y * 100

max = error.argmax()


#print the entries with the most error
print(len(error))
for i in range(0, 1050):
  error = np.delete(error, max)
  max = error.argmax()
  print(error[max])

"""Pickle to save model"""

import pickle
full_path = '/content/sample_data.pkl'

with open(full_path, 'wb') as file:
  pickle.dump(model_2, file)

"""Get most important features"""

#get the most important features
map = {}
columns = Train_X.columns.to_numpy()
for i in range(len(columns)):
  map[columns[i]] = model_2.feature_importances_[i]
print(map)

"""Extra predictions for house on market right now"""

#Cleaning Data
on_sale = pd.read_csv('/content/sample_data/Durham_County (2).csv')
on_sale = on_sale.drop(columns = ['$/SQUARE FEET','PROPERTY TYPE', 'LOCATION', 'HOA/MONTH', 'DAYS ON MARKET', 'SOLD DATE', 'SALE TYPE', 'ADDRESS', 'CITY', 'STATE OR PROVINCE', 'STATUS', 'NEXT OPEN HOUSE START TIME', 'NEXT OPEN HOUSE END TIME', 'URL (SEE https://www.redfin.com/buy-a-home/comparative-market-analysis FOR INFO ON PRICING)', 'SOURCE', 'MLS#', 'FAVORITE', 'INTERESTED'])
on_sale = on_sale.dropna(subset = ['YEAR BUILT', 'BEDS', 'BATHS', 'LOT SIZE', 'SQUARE FEET'])
on_sale.head(5)

#Actual Listing Prices
test_current = on_sale['PRICE']
on_sale = on_sale.drop(columns = ['PRICE'])

#Feature extract again
on_sale['PROPERTY_CRIME'] = on_sale['ZIP OR POSTAL CODE'].map(pc)
on_sale['VIOLENT_CRIME'] = on_sale['ZIP OR POSTAL CODE'].map(vc)
on_sale = on_sale.drop(columns = ['ZIP OR POSTAL CODE'])

on_sale['YEAR'] = 2025
on_sale['MONTH'] = 5
on_sale['HOUSE AGE'] = on_sale['YEAR'] - on_sale['YEAR BUILT']


#getting predictions of on sale houses
#ans = model_2.predict(on_sale)

#calculating errors of predictions
error = (abs(ans - test_current)/test_current)

#get the thirty entries with the most error
max = error.argmax()
for i in range(0, len(error)-1):
  error = np.delete(error, max)
  max = error.argmax()
  print(error[max])
print("\n")


#print the worst predictions
for i in range(0, len(columns)):
  map[columns[i]] = model_2.feature_importances_[i]

print("Mean Absolute Error (MAE): 36307.66795128214")
print("Mean Absolute Percentage Error: 8.817982891807043")